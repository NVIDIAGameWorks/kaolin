{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "occupied-senegal",
   "metadata": {},
   "source": [
    "# Optimizing a mesh using a Differentiable Renderer\n",
    "\n",
    "Differentiable rendering can be used to optimize the underlying 3D properties, like geometry and lighting, by backpropagating gradients from the loss in the image space. In this tutorial, we optimize geometry and texture of a single object based on a dataset of rendered ground truth views. This tutorial demonstrates functionality in `kaolin.render.mesh`, including the key `dibr_rasterization` function.  See detailed [API documentation](https://kaolin.readthedocs.io/en/latest/modules/kaolin.render.mesh.html). Note that this script is didactic and is not meant as a production end-to-end example; for more examples using DIB-R differentiable renderer, see [this repository](https://github.com/nv-tlabs/DIB-R-Single-Image-3D-Reconstruction).\n",
    "\n",
    "In addition, we demonstrate the use of [Kaolin's 3D checkpoints and training visualization](https://kaolin.readthedocs.io/en/latest/modules/kaolin.visualize.html) with the [Omniverse Kaolin App](https://docs.omniverse.nvidia.com/app_kaolin/app_kaolin/user_manual.html).\n",
    "\n",
    "Before starting the tutorial please make sure that to have [examples/samples/rendered_clock.zip](examples/samples/rendered_clock.zip) uncompressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sharp-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q matplotlib\n",
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import kaolin as kal\n",
    "\n",
    "# path to the rendered image (using the data synthesizer)\n",
    "rendered_path = \"../samples/rendered_clock/\"\n",
    "# path to the output logs (readable with the training visualizer in the omniverse app)\n",
    "logs_path = './logs/'\n",
    "\n",
    "# We initialize the timelapse that will store USD for the visualization apps\n",
    "timelapse = kal.visualize.Timelapse(logs_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alike-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epoch = 120 # 40\n",
    "batch_size = 2\n",
    "laplacian_weight = 0.1\n",
    "flat_weight = 0.001\n",
    "image_weight = 0.0 # 0.1\n",
    "mask_weight = 1.\n",
    "texture_lr = 5e-2\n",
    "vertice_lr = 5e-4\n",
    "scheduler_step_size = 20\n",
    "scheduler_gamma = 0.5\n",
    "\n",
    "texture_res = 400\n",
    "\n",
    "# select camera angle for best visualization\n",
    "test_batch_ids = [2, 5, 10]\n",
    "test_batch_size = len(test_batch_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-anderson",
   "metadata": {},
   "source": [
    "# Generating Training Data\n",
    "\n",
    "To optimize a mesh, typical training data includes RGB images and segmentation mask. One way to generate this data is to use the Data Generator in the [Omniverse Kaolin App](https://docs.omniverse.nvidia.com/app_kaolin/app_kaolin/user_manual.html#data-generator). We provide sample output of the app in `examples/samples/`.\n",
    "\n",
    "## Parse synthetic data\n",
    "We first need to parse the synthetic data generated by the omniverse app.\n",
    "The omniverse app generate 1 file per type of data (which can be depth map, rgb image, segmentation map), and an additional metadata json file.\n",
    "\n",
    "The json file contains two main fields:\n",
    "- camera_properties: Contains all the data related to camera setting such as \"clipping_range\", \"horizontal_aperture\", \"focal_length\", \"tf_mat\"\n",
    "- asset_transforms: Those are transformations that are applied by the [Omniverse Kaolin App](https://docs.omniverse.nvidia.com/app_kaolin/app_kaolin/user_manual.html#data-generator), such as rotation / translation between objects or normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "minus-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_views = len(glob.glob(os.path.join(rendered_path,'*_rgb.png')))\n",
    "train_data = []\n",
    "for i in range(num_views):\n",
    "    data = kal.io.render.import_synthetic_view(\n",
    "        rendered_path, i, rgb=True, semantic=True)\n",
    "    train_data.append(data)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                         shuffle=True, pin_memory=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-korea",
   "metadata": {},
   "source": [
    "# Loading the Sphere Template\n",
    "\n",
    "The optimization starts from deforming an input template mesh according to the input image. We will use a sphere template that provides better performance on objects without topological holes. We use \"/kaolin/examples/samples/sphere.obj\" for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "closed-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mesh = kal.io.obj.import_mesh('../samples/sphere.obj', with_materials=True)\n",
    "mesh = kal.io.obj.import_mesh('../samples/bbox.obj', with_materials=True)\n",
    "# the sphere is usually too small (this is fine-tuned for the clock)\n",
    "vertices = mesh.vertices.cuda().unsqueeze(0) * 0.75\n",
    "vertices.requires_grad = True\n",
    "faces = mesh.faces.cuda()\n",
    "uvs = mesh.uvs.cuda().unsqueeze(0)\n",
    "face_uvs_idx = mesh.face_uvs_idx.cuda()\n",
    "\n",
    "\n",
    "face_uvs = kal.ops.mesh.index_vertices_by_faces(uvs, face_uvs_idx).detach()\n",
    "face_uvs.requires_grad = False\n",
    "\n",
    "texture_map = torch.ones((1, 3, texture_res, texture_res), dtype=torch.float, device='cuda',\n",
    "                         requires_grad=True)\n",
    "\n",
    "# The topology of the mesh and the uvs are constant\n",
    "# so we can initialize them on the first iteration only\n",
    "timelapse.add_mesh_batch(\n",
    "    iteration=0,\n",
    "    category='optimized_mesh',\n",
    "    faces_list=[mesh.faces.cpu()],\n",
    "    uvs_list=[mesh.uvs.cpu()],\n",
    "    face_uvs_idx_list=[mesh.face_uvs_idx.cpu()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-findings",
   "metadata": {},
   "source": [
    "# Preparing the losses and regularizer\n",
    "\n",
    "During training we will use different losses:\n",
    "- an image loss: an L1 loss based on RGB image.\n",
    "- a mask loss: an Intersection over Union (IoU) of the segmentation mask with the soft_mask output by DIB-R rasterizer.\n",
    "- a laplacian loss: to avoid deformation that are too strong.\n",
    "- a flat loss: to keep a smooth surface and avoid faces intersecting.\n",
    "\n",
    "For that we need to compute the laplacian matrix and some adjacency information\n",
    "(the face idx of faces connected to each edge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "considerable-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate vertices center as a learnable parameter\n",
    "vertices_init = vertices.clone().detach()\n",
    "vertices_init.requires_grad = False\n",
    "\n",
    "# This is the center of the optimized mesh, separating it as a learnable parameter helps the optimization. \n",
    "vertice_shift = torch.zeros((3,), dtype=torch.float, device='cuda',\n",
    "                            requires_grad=True)\n",
    "\n",
    "def recenter_vertices(vertices, vertice_shift):\n",
    "    \"\"\"Recenter vertices on vertice_shift for better optimization\"\"\"\n",
    "    vertices_min = vertices.min(dim=1, keepdim=True)[0]\n",
    "    vertices_max = vertices.max(dim=1, keepdim=True)[0]\n",
    "    vertices_mid = (vertices_min + vertices_max) / 2\n",
    "    vertices = vertices - vertices_mid + vertice_shift\n",
    "    return vertices\n",
    "\n",
    "\n",
    "nb_faces = faces.shape[0]\n",
    "nb_vertices = vertices_init.shape[1]\n",
    "face_size = 3\n",
    "\n",
    "## Set up auxiliary laplacian matrix for the laplacian loss\n",
    "vertices_laplacian_matrix = kal.ops.mesh.uniform_laplacian(\n",
    "    nb_vertices, faces) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-cross",
   "metadata": {},
   "source": [
    "# Setting up optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "undefined-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_optim  = torch.optim.Adam(params=[vertices, vertice_shift],\n",
    "                                   lr=vertice_lr)\n",
    "texture_optim = torch.optim.Adam(params=[texture_map], lr=texture_lr)\n",
    "vertices_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    vertices_optim,\n",
    "    step_size=scheduler_step_size,\n",
    "    gamma=scheduler_gamma)\n",
    "texture_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    texture_optim,\n",
    "    step_size=scheduler_step_size,\n",
    "    gamma=scheduler_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-suggestion",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This toy tutorial optimizes geometry and texture of the mesh directly to demonstrate losses, rasterization and 3D checkpoints available in Kaolin.\n",
    "\n",
    "These components can be combined with a neural architecture of your choice to learn tasks like image to 3D mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-companion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 0.8718199729919434\n",
      "Epoch 1 - loss: 0.8981483578681946\n",
      "Epoch 2 - loss: 0.8211723566055298\n",
      "Epoch 3 - loss: 0.8186359405517578\n",
      "Epoch 4 - loss: 0.8136696815490723\n",
      "Epoch 5 - loss: 0.8843762278556824\n",
      "Epoch 6 - loss: 0.8320770263671875\n",
      "Epoch 7 - loss: 0.8187311291694641\n",
      "Epoch 8 - loss: 0.8155737519264221\n",
      "Epoch 9 - loss: 0.6959657073020935\n",
      "Epoch 10 - loss: 0.6938010454177856\n",
      "Epoch 11 - loss: 0.6517344117164612\n",
      "Epoch 12 - loss: 0.6130269169807434\n",
      "Epoch 13 - loss: 0.43962541222572327\n",
      "Epoch 14 - loss: 0.46836578845977783\n",
      "Epoch 15 - loss: 0.4059656262397766\n",
      "Epoch 16 - loss: 0.21621978282928467\n",
      "Epoch 17 - loss: 0.3084249496459961\n",
      "Epoch 18 - loss: 0.24882565438747406\n",
      "Epoch 19 - loss: 0.2769607901573181\n",
      "Epoch 20 - loss: 0.16400855779647827\n",
      "Epoch 21 - loss: 0.2737264931201935\n",
      "Epoch 22 - loss: 0.17525559663772583\n",
      "Epoch 23 - loss: 0.16945268213748932\n",
      "Epoch 24 - loss: 0.18069131672382355\n",
      "Epoch 25 - loss: 0.16993984580039978\n",
      "Epoch 26 - loss: 0.15683959424495697\n",
      "Epoch 27 - loss: 0.17266236245632172\n",
      "Epoch 28 - loss: 0.16436856985092163\n",
      "Epoch 29 - loss: 0.13760709762573242\n",
      "Epoch 30 - loss: 0.176626518368721\n",
      "Epoch 31 - loss: 0.14424023032188416\n",
      "Epoch 32 - loss: 0.13694483041763306\n",
      "Epoch 33 - loss: 0.17058850824832916\n",
      "Epoch 34 - loss: 0.1543724536895752\n",
      "Epoch 35 - loss: 0.1877979338169098\n",
      "Epoch 36 - loss: 0.15193307399749756\n",
      "Epoch 37 - loss: 0.15625473856925964\n",
      "Epoch 38 - loss: 0.17441196739673615\n",
      "Epoch 39 - loss: 0.16803671419620514\n",
      "Epoch 40 - loss: 0.16286833584308624\n",
      "Epoch 41 - loss: 0.18004751205444336\n",
      "Epoch 42 - loss: 0.19129808247089386\n",
      "Epoch 43 - loss: 0.15605488419532776\n",
      "Epoch 44 - loss: 0.1233442947268486\n",
      "Epoch 45 - loss: 0.15090487897396088\n",
      "Epoch 46 - loss: 0.13898789882659912\n",
      "Epoch 47 - loss: 0.1711437702178955\n",
      "Epoch 48 - loss: 0.14134743809700012\n",
      "Epoch 49 - loss: 0.13792434334754944\n",
      "Epoch 50 - loss: 0.15585112571716309\n",
      "Epoch 51 - loss: 0.1359969824552536\n",
      "Epoch 52 - loss: 0.12241949886083603\n",
      "Epoch 53 - loss: 0.1441776156425476\n",
      "Epoch 54 - loss: 0.1193762868642807\n",
      "Epoch 55 - loss: 0.16906781494617462\n",
      "Epoch 56 - loss: 0.13067737221717834\n",
      "Epoch 57 - loss: 0.14110234379768372\n",
      "Epoch 58 - loss: 0.14433631300926208\n",
      "Epoch 59 - loss: 0.14284400641918182\n",
      "Epoch 60 - loss: 0.14189636707305908\n",
      "Epoch 61 - loss: 0.15314945578575134\n",
      "Epoch 62 - loss: 0.1694680005311966\n",
      "Epoch 63 - loss: 0.13439799845218658\n",
      "Epoch 64 - loss: 0.1420586258172989\n",
      "Epoch 65 - loss: 0.14680306613445282\n",
      "Epoch 66 - loss: 0.14157289266586304\n",
      "Epoch 67 - loss: 0.13649345934391022\n",
      "Epoch 68 - loss: 0.1705576330423355\n",
      "Epoch 69 - loss: 0.12241756170988083\n",
      "Epoch 70 - loss: 0.14878840744495392\n",
      "Epoch 71 - loss: 0.135968878865242\n",
      "Epoch 72 - loss: 0.13117878139019012\n",
      "Epoch 73 - loss: 0.1536577045917511\n",
      "Epoch 74 - loss: 0.15919974446296692\n",
      "Epoch 75 - loss: 0.1343885213136673\n",
      "Epoch 76 - loss: 0.1285867840051651\n",
      "Epoch 77 - loss: 0.13461671769618988\n",
      "Epoch 78 - loss: 0.1398596167564392\n",
      "Epoch 79 - loss: 0.16410718858242035\n",
      "Epoch 80 - loss: 0.13065491616725922\n",
      "Epoch 81 - loss: 0.13110403716564178\n",
      "Epoch 82 - loss: 0.15872184932231903\n",
      "Epoch 83 - loss: 0.1462564617395401\n",
      "Epoch 84 - loss: 0.16481095552444458\n",
      "Epoch 85 - loss: 0.150045245885849\n",
      "Epoch 86 - loss: 0.1383122056722641\n",
      "Epoch 87 - loss: 0.16076013445854187\n",
      "Epoch 88 - loss: 0.15152297914028168\n",
      "Epoch 89 - loss: 0.16745814681053162\n",
      "Epoch 90 - loss: 0.1461978703737259\n",
      "Epoch 91 - loss: 0.14981737732887268\n",
      "Epoch 92 - loss: 0.14906854927539825\n",
      "Epoch 93 - loss: 0.13862344622612\n",
      "Epoch 94 - loss: 0.1381831169128418\n",
      "Epoch 95 - loss: 0.15442202985286713\n",
      "Epoch 96 - loss: 0.14502644538879395\n",
      "Epoch 97 - loss: 0.1305723488330841\n",
      "Epoch 98 - loss: 0.15078352391719818\n",
      "Epoch 99 - loss: 0.1328822374343872\n",
      "Epoch 100 - loss: 0.14944766461849213\n",
      "Epoch 101 - loss: 0.14241871237754822\n",
      "Epoch 102 - loss: 0.13564234972000122\n",
      "Epoch 103 - loss: 0.13668376207351685\n",
      "Epoch 104 - loss: 0.15558429062366486\n",
      "Epoch 105 - loss: 0.1639994978904724\n",
      "Epoch 106 - loss: 0.1410895586013794\n",
      "Epoch 107 - loss: 0.1453952044248581\n",
      "Epoch 108 - loss: 0.12928088009357452\n",
      "Epoch 109 - loss: 0.13391730189323425\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    for idx, data in enumerate(dataloader):\n",
    "        vertices_optim.zero_grad()\n",
    "        texture_optim.zero_grad()\n",
    "        gt_image = data['rgb'].cuda()\n",
    "        gt_mask = data['semantic'].cuda()\n",
    "        cam_transform = data['metadata']['cam_transform'].cuda()\n",
    "        cam_proj = data['metadata']['cam_proj'].cuda()\n",
    "        \n",
    "        ### Prepare mesh data with projection regarding to camera ###\n",
    "        vertices_batch = recenter_vertices(vertices, vertice_shift)\n",
    "\n",
    "        face_vertices_camera, face_vertices_image, face_normals = \\\n",
    "            kal.render.mesh.prepare_vertices(\n",
    "                vertices_batch.repeat(batch_size, 1, 1),\n",
    "                faces, cam_proj, camera_transform=cam_transform\n",
    "            )\n",
    "\n",
    "        ### Perform Rasterization ###\n",
    "        # Construct attributes that DIB-R rasterizer will interpolate.\n",
    "        # the first is the UVS associated to each face\n",
    "        # the second will make a hard segmentation mask\n",
    "        face_attributes = [\n",
    "            face_uvs.repeat(batch_size, 1, 1, 1),\n",
    "            torch.ones((batch_size, nb_faces, 3, 1), device='cuda')\n",
    "        ]\n",
    "\n",
    "        image_features, soft_mask, face_idx = kal.render.mesh.dibr_rasterization(\n",
    "            gt_image.shape[1], gt_image.shape[2], face_vertices_camera[:, :, :, -1],\n",
    "            face_vertices_image, face_attributes, face_normals[:, :, -1])\n",
    "\n",
    "        # image_features is a tuple in composed of the interpolated attributes of face_attributes\n",
    "        texture_coords, mask = image_features\n",
    "        image = kal.render.mesh.texture_mapping(texture_coords,\n",
    "                                                texture_map.repeat(batch_size, 1, 1, 1), \n",
    "                                                mode='bilinear')\n",
    "        image = torch.clamp(image * mask, 0., 1.)\n",
    "        \n",
    "        ### Compute Losses ###\n",
    "        image_loss = torch.mean(torch.abs(image - gt_image))\n",
    "        mask_loss = kal.metrics.render.mask_iou(soft_mask,\n",
    "                                                gt_mask.squeeze(-1))\n",
    "        # laplacian loss\n",
    "        vertices_mov = vertices - vertices_init\n",
    "        vertices_mov_laplacian = torch.matmul(vertices_laplacian_matrix, vertices_mov)\n",
    "        laplacian_loss = torch.mean(vertices_mov_laplacian ** 2) * nb_vertices * 3\n",
    "\n",
    "        loss = (\n",
    "#             image_loss * image_weight +\n",
    "            mask_loss * mask_weight +\n",
    "            laplacian_loss * laplacian_weight\n",
    "        )\n",
    "        ### Update the mesh ###\n",
    "        loss.backward()\n",
    "        vertices_optim.step()\n",
    "        texture_optim.step()\n",
    "\n",
    "    vertices_scheduler.step()\n",
    "    texture_scheduler.step()\n",
    "    print(f\"Epoch {epoch} - loss: {float(loss)}\")\n",
    "    \n",
    "    ### Write 3D Checkpoints ###\n",
    "    pbr_material = [\n",
    "        {'rgb': kal.io.materials.PBRMaterial(diffuse_texture=torch.clamp(texture_map[0], 0., 1.))}\n",
    "    ]\n",
    "\n",
    "    vertices_batch = recenter_vertices(vertices, vertice_shift)\n",
    "\n",
    "    # We are now adding a new state of the mesh to the timelapse\n",
    "    # we only modify the texture and the vertices position\n",
    "    timelapse.add_mesh_batch(\n",
    "        iteration=epoch,\n",
    "        category='optimized_mesh',\n",
    "        vertices_list=[vertices_batch[0]],\n",
    "        materials_list=pbr_material\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-intranet",
   "metadata": {},
   "source": [
    "# Visualize training\n",
    "\n",
    "You can now use [the Omniverse app](https://docs.omniverse.nvidia.com/app_kaolin/app_kaolin) to visualize the mesh optimization over training by using the training visualizer on \"./logs/\", where we stored the checkpoints.\n",
    "\n",
    "You can also show the rendered image generated by DIB-R and the learned texture map with your 2d images libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # This is similar to a training iteration (without the loss part)\n",
    "    data_batch = [train_data[idx] for idx in test_batch_ids]\n",
    "    cam_transform = torch.stack([data['metadata']['cam_transform'] for data in data_batch], dim=0).cuda()\n",
    "    cam_proj = torch.stack([data['metadata']['cam_proj'] for data in data_batch], dim=0).cuda()\n",
    "\n",
    "    vertices_batch = recenter_vertices(vertices, vertice_shift)\n",
    "\n",
    "    face_vertices_camera, face_vertices_image, face_normals = \\\n",
    "        kal.render.mesh.prepare_vertices(\n",
    "            vertices_batch.repeat(test_batch_size, 1, 1),\n",
    "            faces, cam_proj, camera_transform=cam_transform\n",
    "        )\n",
    "    face_attributes = [\n",
    "        face_uvs.repeat(test_batch_size, 1, 1, 1),\n",
    "        torch.ones((test_batch_size, nb_faces, 3, 1), device='cuda'),\n",
    "    ]\n",
    "\n",
    "    image_features, soft_mask, face_idx = kal.render.mesh.dibr_rasterization(\n",
    "        256, 256, face_vertices_camera[:, :, :, -1],\n",
    "        face_vertices_image, face_attributes, face_normals[:, :, -1])\n",
    "\n",
    "    texture_coords, mask = image_features\n",
    "    image = kal.render.mesh.texture_mapping(texture_coords,\n",
    "                                            texture_map.repeat(test_batch_size, 1, 1, 1), \n",
    "                                            mode='bilinear')\n",
    "    image = torch.clamp(image * mask, 0., 1.)\n",
    "    \n",
    "    ## Display the rendered images\n",
    "    f, axarr = plt.subplots(1, test_batch_size, figsize=(7, 22))\n",
    "    f.subplots_adjust(top=0.99, bottom=0.79, left=0., right=1.4)\n",
    "    f.suptitle('DIB-R rendering', fontsize=30)\n",
    "    for i in range(test_batch_size):\n",
    "        axarr[i].imshow(image[i].cpu().detach())\n",
    "        \n",
    "## Display the texture\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('2D Texture Map', fontsize=30)\n",
    "plt.imshow(torch.clamp(texture_map[0], 0., 1.).cpu().detach().permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9243d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
