{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGCNN PointCloud Classification Tutorial\n",
    "\n",
    "> **Skill level:** _Beginner_\n",
    "\n",
    "Kaolin makes 3D deep learning easy, by providing all the _hard_/_tricky_ parts of coding up a 3D DL application. To get a feel for how easy training classifiers on 3D data has become, here's a quick demo.\n",
    "\n",
    "In this tutorial, we will train a _dgcnn_pointcloud_ classifier, in about five lines of code!\n",
    "\n",
    "We will use the `ModelNet10` dataset for this tutorial. The remainder of this tutorial will be broken down into the following components.\n",
    "\n",
    "- [Downloading ModelNet10](#downloading-modelnet10)\n",
    "- [DataLoading](#dataloading)\n",
    "- [Training the pointcloud classifier](#training-the-pointcloud-classifier)\n",
    "\n",
    "## Downloading ModelNet10\n",
    "\n",
    "Note that the ModelNet10 dataset is provided ONLY for the convenience of academic research. Should you choose to download it, you must adhere to the original terms and copyright notice of the dataset. For convenience, we reproduce the original copyright from the dataset creators.\n",
    "\n",
    "```\n",
    "**Copyright**\n",
    "\n",
    "All CAD models are downloaded from the Internet and the original authors hold the copyright of the CAD models. The label of the data was obtained by us via Amazon Mechanical Turk service and it is provided freely. This dataset is provided for the convenience of academic research only.\n",
    "```\n",
    "\n",
    "The ModelNet10 (10-class subset) dataset is available on the [Princeton ModelNet page](https://modelnet.cs.princeton.edu/). On this page, navigate to the ModelNet10 download link to obtain the dataset. We assume that it is unzipped and extracted to a location `MODELNET_DIRECTORY`.\n",
    "\n",
    "## Warm-up\n",
    "\n",
    "Before all the fun-stuff begins, let us import all necessary functions from `kaolin` and `torch`. A bit more on what the following modules do will become clear as we progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: unable to import datasets/nusc:\n",
      "   No module named 'nuscenes'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from kaolin.datasets import ModelNet\n",
    "from kaolin.models.dgcnn import DGCNNClassifier\n",
    "import kaolin.transforms as tfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloading\n",
    "\n",
    "Kaolin provides convenience functions to load popular 3D datasets (of course, ModelNet10). Assuming you have [installed Kaolin](../../README.md#installation-and-usage), fire up your favourite python interpreter, and execute the following commands.\n",
    "\n",
    "To start, we will define a few important parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnet_path = '/scratch/pushkalkatara/ModelNet10'\n",
    "categories = ['chair', 'sofa']\n",
    "num_points = 1024\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model_path` variable will hold the path to the ModelNet10 dataset. We will use the `categories` variable to specify which classes we want to learn to classify. `num_points` is the number of points we will sample from the mesh when transforming it to a pointcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(inp):\n",
    "    inp.to(device)\n",
    "    return inp\n",
    "\n",
    "transform = tfs.Compose([\n",
    "    to_device,\n",
    "    tfs.TriangleMeshToPointCloud(num_samples=num_points),\n",
    "    tfs.NormalizePointCloud()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command defines a `transform` that first converts a mesh representation to a pointcloud and then _normalizes_ it to be centered at the origin, and have a standard deviation of 1. Much like images, 3D data such as pointclouds need to be normalized for better classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    ModelNet(modelnet_path, categories=categories, split='train', transform=transform),\n",
    "    batch_size=32, shuffle=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew, that was slightly long! But here's what it does. It creates a `DataLoader` object for the `ModelNet10` dataset. In particular, we are interested in loading only the `chair` and `sofa` categories. The `split='train'` argument indicates that we're loading the 'train' split.\n",
    "\n",
    "Similarly, the test dataset can be loaded up as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(\n",
    "    ModelNet(modelnet_path, categories=categories, split='test',transform=transform),\n",
    "    batch_size=16, shuffle=True, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the pointcloud classifier\n",
    "\n",
    "Now that all of the data is ready, we can train our classifier using the `DGCNNClassifier` class provided by Kaolin. The following line of code will train and validate a _PointNet_ classifier, which is probably the simplest of pointcloud neural architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/49 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 43/49 [02:53<00:23,  3.99s/it]"
     ]
    }
   ],
   "source": [
    "model = DGCNNClassifier(device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "epocs = 1\n",
    "\n",
    "for e in range(epocs):\n",
    "    print(\"Epoch: \", e)\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for idx, (data, attributes) in enumerate(tqdm(train_loader)):\n",
    "        category = attributes['category'].cuda()\n",
    "        pred = model(data.cuda())\n",
    "        loss = criterion(pred, category.view(-1))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred_label = torch.argmax(pred, dim=1)\n",
    "        train_accuracy += torch.mean((pred_label == category.view(-1)).float()).detach().cpu().item()\n",
    "        num_batches += 1\n",
    "    print('Train loss:', train_loss / num_batches)\n",
    "    print('Train accuracy:', train_accuracy / num_batches)\n",
    "    \n",
    "    val_loss = 0.\n",
    "    val_accuracy = 0.\n",
    "    num_batches = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (data, attributes) in enumerate(tqdm(val_loader)):\n",
    "            category = attributes['category'].cuda()\n",
    "            pred = model(data.cuda())\n",
    "            loss = criterion(pred, category.view(-1))\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            pred_label = torch.argmax(pred, dim=1)\n",
    "            val_accuracy += torch.mean((pred_label == category.view(-1)).float()).cpu().item()\n",
    "            num_batches += 1\n",
    "        \n",
    "    print('Val loss:', val_loss / num_batches)\n",
    "    print('Val accuracy:', val_accuracy / num_batches)\n",
    "\n",
    "end_time = time.time()\n",
    "print('Training time: {}'.format(end_time - start_time))\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
